<!doctype html>
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Change Capture Using AWS Database Migration Service and Hudi - Apache Hudi</title>
<meta name="description" content="In this blog, we will build an end-end solution for capturing changes from a MySQL instance running on AWS RDS to a Hudi table on S3, using capabilities in the Hudi 0.5.1 release.">

<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="">
<meta property="og:title" content="Change Capture Using AWS Database Migration Service and Hudi">
<meta property="og:url" content="https://hudi.apache.org/change-capture-using-aws/">


  <meta property="og:description" content="In this blog, we will build an end-end solution for capturing changes from a MySQL instance running on AWS RDS to a Hudi table on S3, using capabilities in the Hudi 0.5.1 release.">











<!-- end _includes/seo.html -->


<!--<link href="/feed.xml" type="application/atom+xml" rel="alternate" title=" Feed">-->

<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">

<!--[if IE]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->



<link rel="icon" type="image/x-icon" href="/assets/images/favicon.ico">
<link rel="stylesheet" href="/assets/css/font-awesome.min.css">

  </head>

  <body class="layout--single">
    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    <div class="masthead">
  <div class="masthead__inner-wrap" id="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
          <a class="site-logo" href="/">
              <div style="width: 150px; height: 40px">
              </div>
          </a>
        
        <a class="site-title" href="/">
          
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/docs/quick-start-guide.html" target="_self" >Documentation</a>
            </li><li class="masthead__menu-item">
              <a href="/community.html" target="_self" >Community</a>
            </li><li class="masthead__menu-item">
              <a href="/blog.html" target="_self" >Blog</a>
            </li><li class="masthead__menu-item">
              <a href="https://cwiki.apache.org/confluence/display/HUDI/FAQ" target="_blank" >FAQ</a>
            </li><li class="masthead__menu-item">
              <a href="/releases.html" target="_self" >Releases</a>
            </li></ul>
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>
<!--
<p class="notice--warning" style="margin: 0 !important; text-align: center !important;"><strong>Note:</strong> This site is work in progress, if you notice any issues, please <a target="_blank" href="https://github.com/apache/incubator-hudi/issues">Report on Issue</a>.
  Click <a href="/"> here</a> back to old site.</p>
-->

    <div class="initial-content">
      <div id="main" role="main">
  

  <div class="sidebar sticky">

  
    <div itemscope itemtype="https://schema.org/Person">

  <div class="author__content">
    
      <h3 class="author__name" itemprop="name">Quick Links</h3>
    
    
      <div class="author__bio" itemprop="description">
        <p>Hudi <em>ingests</em> &amp; <em>manages</em> storage of large analytical datasets over DFS.</p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <ul class="author__urls social-icons">
      
        
          <li><a href="/docs/quick-start-guide" target="_self" rel="nofollow noopener noreferrer"><i class="fa fa-book" aria-hidden="true"></i> Documentation</a></li>

          
        
          <li><a href="https://cwiki.apache.org/confluence/display/HUDI" target="_blank" rel="nofollow noopener noreferrer"><i class="fa fa-wikipedia-w" aria-hidden="true"></i> Technical Wiki</a></li>

          
        
          <li><a href="/contributing" target="_self" rel="nofollow noopener noreferrer"><i class="fa fa-thumbs-o-up" aria-hidden="true"></i> Contribution Guide</a></li>

          
        
          <li><a href="https://join.slack.com/t/apache-hudi/shared_invite/enQtODYyNDAxNzc5MTg2LTE5OTBlYmVhYjM0N2ZhOTJjOWM4YzBmMWU2MjZjMGE4NDc5ZDFiOGQ2N2VkYTVkNzU3ZDQ4OTI1NmFmYWQ0NzE" target="_blank" rel="nofollow noopener noreferrer"><i class="fa fa-slack" aria-hidden="true"></i> Join on Slack</a></li>

          
        
          <li><a href="https://github.com/apache/incubator-hudi" target="_blank" rel="nofollow noopener noreferrer"><i class="fa fa-github" aria-hidden="true"></i> Fork on GitHub</a></li>

          
        
          <li><a href="https://issues.apache.org/jira/projects/HUDI/summary" target="_blank" rel="nofollow noopener noreferrer"><i class="fa fa-navicon" aria-hidden="true"></i> Report Issues</a></li>

          
        
          <li><a href="/security" target="_self" rel="nofollow noopener noreferrer"><i class="fa fa-navicon" aria-hidden="true"></i> Report Security Issues</a></li>

          
        
      
    </ul>
  </div>
</div>

  

  
  </div>


  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <!-- Look the author details up from the site config. -->
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">Change Capture Using AWS Database Migration Service and Hudi
</h1>
          <!-- Output author details if some exist. -->
          <div class="page__author"><a href="https://cwiki.apache.org/confluence/display/~vinoth">Vinoth Chandar</a> posted on <time datetime="2020-01-20">January 20, 2020</time></span>
        </header>
      

      <section class="page__content" itemprop="text">
        
          <style>
            .page {
              padding-right: 0 !important;
            }
          </style>
        
        <p>One of the core use-cases for Apache Hudi is enabling seamless, efficient database ingestion to your data lake. Even though a lot has been talked about and even users already adopting this model, content on how to go about this is sparse.</p>

<p>In this blog, we will build an end-end solution for capturing changes from a MySQL instance running on AWS RDS to a Hudi table on S3, using capabilities in the Hudi  <strong>0.5.1 release</strong></p>

<p>We can break up the problem into two pieces.</p>

<ol>
  <li><strong>Extracting change logs from MySQL</strong>  : Surprisingly, this is still a pretty tricky problem to solve and often Hudi users get stuck here. Thankfully, at-least for AWS users, there is a  <a href="https://aws.amazon.com/dms/">Database Migration service</a>  (DMS for short), that does this change capture and uploads them as parquet files on S3</li>
  <li><strong>Applying these change logs to your data lake table</strong>  : Once there are change logs in some form, the next step is to apply them incrementally to your table. This mundane task can be fully automated using the Hudi  <a href="http://hudi.apache.org/docs/writing_data.html#deltastreamer">DeltaStreamer</a>  tool.</li>
</ol>

<p>The actual end-end architecture looks something like this.
<img src="/assets/images/blog/change-capture-architecture.png" alt="enter image description here" /></p>

<p>Letâ€™s now illustrate how one can accomplish this using a simple <em>orders</em> table, stored in MySQL (these instructions should broadly apply to other database engines like Postgres, or Aurora as well, though SQL/Syntax may change)</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>CREATE DATABASE hudi_dms;
USE hudi_dms;
 
CREATE TABLE orders(
   order_id INTEGER,
   order_qty INTEGER,
   customer_name VARCHAR(100),
   updated_at TIMESTAMP DEFAULT NOW() ON UPDATE NOW(),
   created_at TIMESTAMP DEFAULT NOW(),
   CONSTRAINT orders_pk PRIMARY KEY(order_id)
);
 
INSERT INTO orders(order_id, order_qty, customer_name) VALUES(1, 10, 'victor');
INSERT INTO orders(order_id, order_qty, customer_name) VALUES(2, 20, 'peter');
</code></pre></div></div>

<p>In the table, <em>order_id</em> is the primary key which will be enforced on the Hudi table as well. Since a batch of change records can contain changes to the same primary key, we also include <em>updated_at</em> and <em>created_at</em> fields, which are kept upto date as writes happen to the table.</p>

<h3 id="extracting-change-logs-from-mysql">Extracting Change logs from MySQL</h3>

<p>Before we can configure DMS, we first need to <a href="https://aws.amazon.com/premiumsupport/knowledge-center/enable-binary-logging-aurora/">prepare the MySQL instance</a>  for change capture, by ensuring backups are enabled and binlog is turned on.
<img src="/assets/images/blog/change-logs-mysql.png" alt="" /></p>

<p>Now, proceed to create endpoints in DMS that capture MySQL data and  <a href="https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.S3.html">store in S3, as parquet files</a>.</p>

<ul>
  <li>Source <em>hudi-source-db</em> endpoint, points to the DB server and provides basic authentication details</li>
  <li>Target <em>parquet-s3</em> endpoint, points to the bucket and folder on s3 to store the change logs records as parquet files
<img src="/assets/images/blog/s3-endpoint-configuration-1.png" alt="" />
<img src="/assets/images/blog/s3-endpoint-configuration-2.png" alt="" />
<img src="/assets/images/blog/s3-endpoint-list.png" alt="" /></li>
</ul>

<p>Then proceed to create a migration task, as below. Give it a name, connect the source to the target and be sure to pick the right <em>Migration type</em> as shown below, to ensure ongoing changes are continuously replicated to S3. Also make sure to specify, the rules using which DMS decides which MySQL schema/tables to replicate. In this example, we simply whitelist <em>orders</em> table under the <em>hudi_dms</em> schema, as specified in the table SQL above.</p>

<p><img src="/assets/images/blog/s3-migration-task-1.png" alt="" />
<img src="/assets/images/blog/s3-migration-task-2.png" alt="" /></p>

<p>Starting the DMS task and should result in an initial load, like below.</p>

<p><img src="/assets/images/blog/dms-task.png" alt="" /></p>

<p>Simply reading the raw initial load file, shoud give the same values as the upstream table</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>scala&gt; spark.read.parquet("s3://hudi-dms-demo/orders/hudi_dms/orders/*").sort("updated_at").show
 
+--------+---------+-------------+-------------------+-------------------+
|order_id|order_qty|customer_name|         updated_at|         created_at|
+--------+---------+-------------+-------------------+-------------------+
|       2|       10|        peter|2020-01-20 20:12:22|2020-01-20 20:12:22|
|       1|       10|       victor|2020-01-20 20:12:31|2020-01-20 20:12:31|
+--------+---------+-------------+-------------------+-------------------+
</code></pre></div></div>

<h2 id="applying-change-logs-using-hudi-deltastreamer">Applying Change Logs using Hudi DeltaStreamer</h2>

<p>Now, we are ready to start consuming the change logs. Hudi DeltaStreamer runs as Spark job on your favorite workflow scheduler (it also supports a continuous mode using <em>â€“continuous</em> flag, where it runs as a long running Spark job), that tails a given path on S3 (or any DFS implementation) for new files and can issue an <em>upsert</em> to a target hudi dataset. The tool automatically checkpoints itself and thus to repeatedly ingest, all one needs to do is to keep executing the DeltaStreamer periodically.</p>

<p>With an initial load already on S3, we then run the following command (deltastreamer command, here on) to ingest the full load first and create a Hudi dataset on S3.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>spark-submit --class org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer  \
  --packages org.apache.spark:spark-avro_2.11:2.4.4 \
  --master yarn --deploy-mode client \
  hudi-utilities-bundle_2.11-0.5.1-SNAPSHOT.jar \
  --table-type COPY_ON_WRITE \
  --source-ordering-field updated_at \
  --source-class org.apache.hudi.utilities.sources.ParquetDFSSource \
  --target-base-path s3://hudi-dms-demo/hudi_orders --target-table hudi_orders \
  --transformer-class org.apache.hudi.utilities.transform.AWSDmsTransformer \
  --payload-class org.apache.hudi.payload.AWSDmsAvroPayload \
  --hoodie-conf hoodie.datasource.write.recordkey.field=order_id,hoodie.datasource.write.partitionpath.field=customer_name,hoodie.deltastreamer.source.dfs.root=s3://hudi-dms-demo/orders/hudi_dms/orders
</code></pre></div></div>

<p>A few things are going on here</p>

<ul>
  <li>First, we specify the <em>â€“table-type</em> as COPY_ON_WRITE. Hudi also supports another _MERGE_ON_READ ty_pe you can use if you choose from.</li>
  <li>To handle cases where the input parquet files contain multiple updates/deletes or insert/updates to the same record, we use <em>updated_at</em> as the ordering field. This ensures that the change record which has the latest timestamp will be reflected in Hudi.</li>
  <li>We specify a target base path and a table table, all needed for creating and writing to the Hudi table</li>
  <li>We use a special payload class - <em>AWSDMSAvroPayload</em> , to handle the different change operations correctly. The parquet files generated have an <em>Op</em> field, that indicates whether a given change record is an insert (I), delete (D) or update (U) and the payload implementation uses this field to decide how to handle a given change record.</li>
  <li>You may also notice a special transformer class <em>AWSDmsTransformer</em> , being specified. The reason here is tactical, but important. The initial load file does not contain an <em>Op</em> field, so this adds one to Hudi table schema additionally.</li>
  <li>Finally, we specify the record key for the Hudi table as same as the upstream table. Then we specify partitioning by <em>customer_name</em>  and also the root of the DMS output.</li>
</ul>

<p>Once the command is run, the Hudi table should be created and have same records as the upstream table (with all the _hoodie fields as well).</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>scala&gt; spark.read.format("org.apache.hudi").load("s3://hudi-dms-demo/hudi_orders/*/*.parquet").show
+-------------------+--------------------+------------------+----------------------+--------------------+--------+---------+-------------+-------------------+-------------------+---+
|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|order_id|order_qty|customer_name|         updated_at|         created_at| Op|
+-------------------+--------------------+------------------+----------------------+--------------------+--------+---------+-------------+-------------------+-------------------+---+
|     20200120205028|  20200120205028_0_1|                 2|                 peter|af9a2525-a486-40e...|       2|       10|        peter|2020-01-20 20:12:22|2020-01-20 20:12:22|   |
|     20200120205028|  20200120205028_1_1|                 1|                victor|8e431ece-d51c-4c7...|       1|       10|       victor|2020-01-20 20:12:31|2020-01-20 20:12:31|   |
+-------------------+--------------------+------------------+----------------------+--------------------+--------+---------+-------------+-------------------+-------------------+---+
</code></pre></div></div>

<p>Now, letâ€™s do an insert and an update</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>INSERT INTO orders(order_id, order_qty, customer_name) VALUES(3, 30, 'sandy');
UPDATE orders set order_qty = 20 where order_id = 2;
</code></pre></div></div>

<p>This will add a new parquet file to the DMS output folder and when the deltastreamer command is run again, it will go ahead and apply these to the Hudi table.</p>

<p>So, querying the Hudi table now would yield 3 rows and the <em>hoodie_commit_time</em> accurately reflects when these writes happened. You can notice that order_qty for order_id=2, is updated from 10 to 20!</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>+-------------------+--------------------+------------------+----------------------+--------------------+---+--------+---------+-------------+-------------------+-------------------+
|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name| Op|order_id|order_qty|customer_name|         updated_at|         created_at|
+-------------------+--------------------+------------------+----------------------+--------------------+---+--------+---------+-------------+-------------------+-------------------+
|     20200120211526|  20200120211526_0_1|                 2|                 peter|af9a2525-a486-40e...|  U|       2|       20|        peter|2020-01-20 21:11:47|2020-01-20 20:12:22|
|     20200120211526|  20200120211526_1_1|                 3|                 sandy|566eb34a-e2c5-44b...|  I|       3|       30|        sandy|2020-01-20 21:11:24|2020-01-20 21:11:24|
|     20200120205028|  20200120205028_1_1|                 1|                victor|8e431ece-d51c-4c7...|   |       1|       10|       victor|2020-01-20 20:12:31|2020-01-20 20:12:31|
+-------------------+--------------------+------------------+----------------------+--------------------+---+--------+---------+-------------+-------------------+-------------------+
</code></pre></div></div>

<p>A nice debugging aid would be read all of the DMS output now and sort it by update_at, which should give us a sequence of changes that happened on the upstream table. As we can see, the Hudi table above is a compacted snapshot of this raw change log.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>+----+--------+---------+-------------+-------------------+-------------------+
|  Op|order_id|order_qty|customer_name|         updated_at|         created_at|
+----+--------+---------+-------------+-------------------+-------------------+
|null|       2|       10|        peter|2020-01-20 20:12:22|2020-01-20 20:12:22|
|null|       1|       10|       victor|2020-01-20 20:12:31|2020-01-20 20:12:31|
|   I|       3|       30|        sandy|2020-01-20 21:11:24|2020-01-20 21:11:24|
|   U|       2|       20|        peter|2020-01-20 21:11:47|2020-01-20 20:12:22|
+----+--------+---------+-------------+-------------------+-------------------+
</code></pre></div></div>

<p>Initial load with no <em>Op</em> field value , followed by an insert and an update.</p>

<p>Now, lets do deletes an inserts</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>DELETE FROM orders WHERE order_id = 2;
INSERT INTO orders(order_id, order_qty, customer_name) VALUES(4, 40, 'barry');
INSERT INTO orders(order_id, order_qty, customer_name) VALUES(5, 50, 'nathan');
</code></pre></div></div>

<p>This should result in more files on S3, written by DMS , which the DeltaStreamer command will continue to process incrementally (i.e only the newly written files are read each time)</p>

<p><img src="/assets/images/blog/dms-demo-files.png" alt="" /></p>

<p>Running the deltastreamer command again, would result in the follow state for the Hudi table. You can notice the two new records and that the <em>order_id=2</em> is now gone</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>+-------------------+--------------------+------------------+----------------------+--------------------+---+--------+---------+-------------+-------------------+-------------------+
|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name| Op|order_id|order_qty|customer_name|         updated_at|         created_at|
+-------------------+--------------------+------------------+----------------------+--------------------+---+--------+---------+-------------+-------------------+-------------------+
|     20200120212522|  20200120212522_1_1|                 5|                nathan|3da94b20-c70b-457...|  I|       5|       50|       nathan|2020-01-20 21:23:00|2020-01-20 21:23:00|
|     20200120212522|  20200120212522_2_1|                 4|                 barry|8cc46715-8f0f-48a...|  I|       4|       40|        barry|2020-01-20 21:22:49|2020-01-20 21:22:49|
|     20200120211526|  20200120211526_1_1|                 3|                 sandy|566eb34a-e2c5-44b...|  I|       3|       30|        sandy|2020-01-20 21:11:24|2020-01-20 21:11:24|
|     20200120205028|  20200120205028_1_1|                 1|                victor|8e431ece-d51c-4c7...|   |       1|       10|       victor|2020-01-20 20:12:31|2020-01-20 20:12:31|
+-------------------+--------------------+------------------+----------------------+--------------------+---+--------+---------+-------------+-------------------+-------------------+ Our little informal change log query yields the following.

+----+--------+---------+-------------+-------------------+-------------------+
|  Op|order_id|order_qty|customer_name|         updated_at|         created_at|
+----+--------+---------+-------------+-------------------+-------------------+
|null|       2|       10|        peter|2020-01-20 20:12:22|2020-01-20 20:12:22|
|null|       1|       10|       victor|2020-01-20 20:12:31|2020-01-20 20:12:31|
|   I|       3|       30|        sandy|2020-01-20 21:11:24|2020-01-20 21:11:24|
|   U|       2|       20|        peter|2020-01-20 21:11:47|2020-01-20 20:12:22|
|   D|       2|       20|        peter|2020-01-20 21:11:47|2020-01-20 20:12:22|
|   I|       4|       40|        barry|2020-01-20 21:22:49|2020-01-20 21:22:49|
|   I|       5|       50|       nathan|2020-01-20 21:23:00|2020-01-20 21:23:00|
+----+--------+---------+-------------+-------------------+-------------------+
</code></pre></div></div>

<p>Note that the delete and update have the same <em>updated_at,</em> value. thus it can very well order differently here.. In short this way of looking at the changelog has its caveats. For a true changelog of the Hudi table itself, you can issue an <a href="http://hudi.apache.org/docs/querying_data.html">incremental query</a>.</p>

<p>And Life goes on â€¦.. Hope this was useful to all the data engineers out there!</p>


      </section>

      <a href="#masthead__inner-wrap" class="back-to-top">Back to top &uarr;</a>


      

    </div>

  </article>

</div>

    </div>

    <div class="page__footer">
      <footer>
        
<div class="row">
  <div class="col-lg-12 footer">
    <p>
      <a class="footer-link-img" href="https://apache.org">
        <img width="250px" src="/assets/images/asf_logo.svg" alt="The Apache Software Foundation">
      </a>
    </p>
    <p>
      Copyright &copy; <span id="copyright-year">2019</span> <a href="https://apache.org">The Apache Software Foundation</a>, Licensed under the Apache License, Version 2.0.
      Hudi, Apache and the Apache feather logo are trademarks of The Apache Software Foundation. <a href="/docs/privacy">Privacy Policy</a>
      <br>
      Apache Hudi is an effort undergoing incubation at The Apache Software Foundation (ASF), sponsored by the <a href="http://incubator.apache.org/">Apache Incubator</a>.
      Incubation is required of all newly accepted projects until a further review indicates that the infrastructure, communications, and decision making process have
      stabilized in a manner consistent with other successful ASF projects. While incubation status is not necessarily a
      reflection of the completeness or stability of the code, it does indicate that the project has yet to be fully endorsed by the ASF.
    </p>
  </div>
</div>
      </footer>
    </div>

    
<script src="/assets/js/main.min.js"></script>


  </body>
</html>